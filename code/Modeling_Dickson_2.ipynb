{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0669e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import datetime as dt\n",
    "from dateutil.parser import *\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from numpy import argmax\n",
    "import warnings\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score,classification_report, roc_auc_score,accuracy_score, \\\n",
    "precision_score, f1_score, recall_score, roc_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rc('xtick', labelsize=12) \n",
    "plt.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ac99645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression = pd.read_csv('../data/merged_train_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27b2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_regression = pd.read_csv('../data/merged_test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f789e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_multi = pd.read_csv('../data/merged_train_multi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e844ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test_multi = pd.read_csv('../data/merged_test_multi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d022cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.948107\n",
       "1    0.051893\n",
       "Name: wnvpresent, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = df_regression.drop(['wnvpresent'],axis=1)\n",
    "y = df_regression['wnvpresent']\n",
    "\n",
    "#x = merged_train_multi.drop(['wnvpresent'],axis=1)\n",
    "#y = merged_train_multi['wnvpresent']\n",
    "\n",
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_sc = ss.fit_transform(x_train)\n",
    "x_test_sc = ss.transform(x_test)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "x_train_sc, y_train = sm.fit_resample(x_train_sc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d43ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "models = {'lr': LogisticRegression(max_iter=5_000, random_state=42, solver='saga'),\n",
    "          'rf': RandomForestClassifier(random_state=42),\n",
    "          'gb': GradientBoostingClassifier(random_state=42),\n",
    "          'et': ExtraTreesClassifier(random_state=42),\n",
    "          'ada': AdaBoostClassifier(random_state=42),\n",
    "          'svm': SVC(random_state=42, probability=True),\n",
    "          'xg': XGBClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba6eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input model\n",
    "def get_model_scores(model_name,\n",
    "                     mod, \n",
    "                     mod_params={}, \n",
    "                     grid_search=False):\n",
    "    \n",
    "    \"\"\"Function accepts following inputs:\n",
    "    Name of model (str), model to be used (str), \n",
    "    model params(dict, optional), grid_seach(boolean, optional)\n",
    "    If grid_search is True, then please also input the relevant \n",
    "    params for GridSearching\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty dict for appending results\n",
    "    results = {}\n",
    "    \n",
    "    # instantiate pipe\n",
    "    pipe = Pipeline([\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    # check if GridSearch true or false\n",
    "    if grid_search:\n",
    "        \n",
    "        # combine vectorizer and mod params together\n",
    "        gs_params = {}\n",
    "        gs_params.update(mod_params)\n",
    "        \n",
    "        # instantiate GridSearchCV\n",
    "        gs = GridSearchCV(pipe, \n",
    "                          param_grid=gs_params,\n",
    "                          cv=5, \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1)\n",
    "        \n",
    "        # fit model\n",
    "        gs.fit(x_train_sc, y_train)\n",
    "        pipe = gs\n",
    "        \n",
    "    else:\n",
    "        # else fit model\n",
    "        pipe.fit(x_train_sc, y_train)\n",
    "    \n",
    "    # create predictions and confusion matrix\n",
    "    predictions = pipe.predict(x_test_sc)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    y_test_pred_prob = pipe.predict_proba(x_test_sc)[:,1]\n",
    "    y_train_pred_prob = pipe.predict_proba(x_train_sc)[:,1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    # Retrieve metrics and add to results\n",
    "    results['model_name'] = model_name\n",
    "    results['model'] = mod\n",
    "    results['train_score'] = pipe.score(x_train_sc, y_train)\n",
    "    results['test_score'] = pipe.score(x_test_sc, y_test)\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, \n",
    "                                     predictions)\n",
    "    \n",
    "    results['specificity'] = tn/(tn + fp)\n",
    "    \n",
    "    results['precision'] = precision_score(y_test, \n",
    "                                           predictions)\n",
    "    \n",
    "    results['train_auc'] = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    results['test_auc'] = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    results['is_tuned'] = grid_search\n",
    "    \n",
    "    if grid_search:\n",
    "        print('BEST PARAMS-->')\n",
    "        display(pipe.best_params_)\n",
    "    \n",
    "    # add results to list for model evaluation later\n",
    "    model_eval.append(results)\n",
    "    \n",
    "    print('METRICS-->')\n",
    "    display(results)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    \n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce484811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list to store results\n",
    "model_eval =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e455ed0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'lr_base',\n",
       " 'model': 'lr',\n",
       " 'train_score': 0.8057329462989841,\n",
       " 'test_score': 0.7541254125412541,\n",
       " 'recall': 0.5887096774193549,\n",
       " 'specificity': 0.7630434782608696,\n",
       " 'precision': 0.11812297734627832,\n",
       " 'train_auc': 0.8751527212826059,\n",
       " 'test_auc': 0.7461448106591865,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1755\n",
      "False Positives: 545\n",
      "False Negatives: 51\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "lr_reg = get_model_scores('lr_base', 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1077be5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'svm_base',\n",
       " 'model': 'svm',\n",
       " 'train_score': 0.9130624092888244,\n",
       " 'test_score': 0.8407590759075908,\n",
       " 'recall': 0.4274193548387097,\n",
       " 'specificity': 0.8630434782608696,\n",
       " 'precision': 0.14402173913043478,\n",
       " 'train_auc': 0.9712810577160058,\n",
       " 'test_auc': 0.7514708976157083,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1985\n",
      "False Positives: 315\n",
      "False Negatives: 71\n",
      "True Positives: 53\n"
     ]
    }
   ],
   "source": [
    "svm_reg = get_model_scores('svm_base', 'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5ee4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:43:11] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'xg_base',\n",
       " 'model': 'xg',\n",
       " 'train_score': 0.9776487663280116,\n",
       " 'test_score': 0.9426567656765676,\n",
       " 'recall': 0.08064516129032258,\n",
       " 'specificity': 0.9891304347826086,\n",
       " 'precision': 0.2857142857142857,\n",
       " 'train_auc': 0.9971802490304831,\n",
       " 'test_auc': 0.8572072230014025,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2275\n",
      "False Positives: 25\n",
      "False Negatives: 114\n",
      "True Positives: 10\n"
     ]
    }
   ],
   "source": [
    "xg_reg = get_model_scores('xg_base', 'xg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f74bbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'et_base',\n",
       " 'model': 'et',\n",
       " 'train_score': 0.9918722786647315,\n",
       " 'test_score': 0.941006600660066,\n",
       " 'recall': 0.07258064516129033,\n",
       " 'specificity': 0.9878260869565217,\n",
       " 'precision': 0.24324324324324326,\n",
       " 'train_auc': 0.9998393056131918,\n",
       " 'test_auc': 0.6866374474053296,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2272\n",
      "False Positives: 28\n",
      "False Negatives: 115\n",
      "True Positives: 9\n"
     ]
    }
   ],
   "source": [
    "et_reg = get_model_scores('et_base', 'et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b831c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'lr_base',\n",
       "  'model': 'lr',\n",
       "  'train_score': 0.8057329462989841,\n",
       "  'test_score': 0.7541254125412541,\n",
       "  'recall': 0.5887096774193549,\n",
       "  'specificity': 0.7630434782608696,\n",
       "  'precision': 0.11812297734627832,\n",
       "  'train_auc': 0.8751527212826059,\n",
       "  'test_auc': 0.7461448106591865,\n",
       "  'is_tuned': False},\n",
       " {'model_name': 'svm_base',\n",
       "  'model': 'svm',\n",
       "  'train_score': 0.9130624092888244,\n",
       "  'test_score': 0.8407590759075908,\n",
       "  'recall': 0.4274193548387097,\n",
       "  'specificity': 0.8630434782608696,\n",
       "  'precision': 0.14402173913043478,\n",
       "  'train_auc': 0.9712810577160058,\n",
       "  'test_auc': 0.7514708976157083,\n",
       "  'is_tuned': False},\n",
       " {'model_name': 'xg_base',\n",
       "  'model': 'xg',\n",
       "  'train_score': 0.9776487663280116,\n",
       "  'test_score': 0.9426567656765676,\n",
       "  'recall': 0.08064516129032258,\n",
       "  'specificity': 0.9891304347826086,\n",
       "  'precision': 0.2857142857142857,\n",
       "  'train_auc': 0.9971802490304831,\n",
       "  'test_auc': 0.8572072230014025,\n",
       "  'is_tuned': False},\n",
       " {'model_name': 'et_base',\n",
       "  'model': 'et',\n",
       "  'train_score': 0.9918722786647315,\n",
       "  'test_score': 0.941006600660066,\n",
       "  'recall': 0.07258064516129033,\n",
       "  'specificity': 0.9878260869565217,\n",
       "  'precision': 0.24324324324324326,\n",
       "  'train_auc': 0.9998393056131918,\n",
       "  'test_auc': 0.6866374474053296,\n",
       "  'is_tuned': False}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c275f0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.948107\n",
       "1    0.051893\n",
       "Name: wnvpresent, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = df_regression.drop(['wnvpresent'],axis=1)\n",
    "#y = df_regression['wnvpresent']\n",
    "\n",
    "x = merged_train_multi.drop(['wnvpresent'],axis=1)\n",
    "y = merged_train_multi['wnvpresent']\n",
    "\n",
    "display(y.value_counts(normalize=True))\n",
    "\n",
    "x_train, x_test, y_train, y_test= train_test_split(x,y,random_state=42)\n",
    "\n",
    "ss = StandardScaler()\n",
    "x_train_sc = ss.fit_transform(x_train)\n",
    "x_test_sc = ss.transform(x_test)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "x_train_sc, y_train = sm.fit_resample(x_train_sc,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "db71b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input model\n",
    "def get_model_scores(model_name,\n",
    "                     mod, \n",
    "                     mod_params={}, \n",
    "                     grid_search=False):\n",
    "    \n",
    "    \"\"\"Function accepts following inputs:\n",
    "    Name of model (str), model to be used (str), \n",
    "    model params(dict, optional), grid_seach(boolean, optional)\n",
    "    If grid_search is True, then please also input the relevant \n",
    "    params for GridSearching\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty dict for appending results\n",
    "    results = {}\n",
    "    \n",
    "    # instantiate pipe\n",
    "    pipe = Pipeline([\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    # check if GridSearch true or false\n",
    "    if grid_search:\n",
    "        \n",
    "        # combine vectorizer and mod params together\n",
    "        gs_params = {}\n",
    "        gs_params.update(mod_params)\n",
    "        \n",
    "        # instantiate GridSearchCV\n",
    "        gs = GridSearchCV(pipe, \n",
    "                          param_grid=gs_params,\n",
    "                          cv=5, \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1,\n",
    "                          scoring ='recall'\n",
    "                          \n",
    "                         )\n",
    "        \n",
    "        # fit model\n",
    "        gs.fit(x_train_sc, y_train)\n",
    "        pipe = gs\n",
    "        \n",
    "    else:\n",
    "        # else fit model\n",
    "        pipe.fit(x_train_sc, y_train)\n",
    "    \n",
    "    # create predictions and confusion matrix\n",
    "    predictions = pipe.predict(x_test_sc)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    y_test_pred_prob = pipe.predict_proba(x_test_sc)[:,1]\n",
    "    y_train_pred_prob = pipe.predict_proba(x_train_sc)[:,1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    # Retrieve metrics and add to results\n",
    "    results['model_name'] = model_name\n",
    "    results['model'] = mod\n",
    "    results['train_score'] = pipe.score(x_train_sc, y_train)\n",
    "    results['test_score'] = pipe.score(x_test_sc, y_test)\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, \n",
    "                                     predictions)\n",
    "    \n",
    "    results['specificity'] = tn/(tn + fp)\n",
    "    \n",
    "    results['precision'] = precision_score(y_test, \n",
    "                                           predictions)\n",
    "    \n",
    "    results['train_auc'] = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    results['test_auc'] = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    results['is_tuned'] = grid_search\n",
    "    \n",
    "    if grid_search:\n",
    "        print('BEST PARAMS-->')\n",
    "        display(pipe.best_params_)\n",
    "    \n",
    "    # add results to list for model evaluation later\n",
    "    model_eval_2.append(results)\n",
    "    \n",
    "    print('METRICS-->')\n",
    "    display(results)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    \n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "22c01948",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1e808d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'lr_base',\n",
       " 'model': 'lr',\n",
       " 'train_score': 0.8259796806966618,\n",
       " 'test_score': 0.775990099009901,\n",
       " 'recall': 0.6048387096774194,\n",
       " 'specificity': 0.7852173913043479,\n",
       " 'precision': 0.13181019332161686,\n",
       " 'train_auc': 0.8874044228083443,\n",
       " 'test_auc': 0.7729926367461429,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1806\n",
      "False Positives: 494\n",
      "False Negatives: 49\n",
      "True Positives: 75\n"
     ]
    }
   ],
   "source": [
    "lr_base = get_model_scores('lr_base', 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ade7c5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'svm_base',\n",
       " 'model': 'svm',\n",
       " 'train_score': 0.9298984034833091,\n",
       " 'test_score': 0.8535478547854786,\n",
       " 'recall': 0.45161290322580644,\n",
       " 'specificity': 0.8752173913043478,\n",
       " 'precision': 0.16326530612244897,\n",
       " 'train_auc': 0.9783968162352202,\n",
       " 'test_auc': 0.777312412342216,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2013\n",
      "False Positives: 287\n",
      "False Negatives: 68\n",
      "True Positives: 56\n"
     ]
    }
   ],
   "source": [
    "svm_base = get_model_scores('svm_base', 'svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "eba6f4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:00:46] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'xg_base',\n",
       " 'model': 'xg',\n",
       " 'train_score': 0.978011611030479,\n",
       " 'test_score': 0.9422442244224423,\n",
       " 'recall': 0.0967741935483871,\n",
       " 'specificity': 0.9878260869565217,\n",
       " 'precision': 0.3,\n",
       " 'train_auc': 0.9974362836276467,\n",
       " 'test_auc': 0.8528962131837308,\n",
       " 'is_tuned': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2272\n",
      "False Positives: 28\n",
      "False Negatives: 112\n",
      "True Positives: 12\n"
     ]
    }
   ],
   "source": [
    "xg_base = get_model_scores('xg_base', 'xg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c4656609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': 'lr_base',\n",
       "  'model': 'lr',\n",
       "  'train_score': 0.8259796806966618,\n",
       "  'test_score': 0.775990099009901,\n",
       "  'recall': 0.6048387096774194,\n",
       "  'specificity': 0.7852173913043479,\n",
       "  'precision': 0.13181019332161686,\n",
       "  'train_auc': 0.8874044228083443,\n",
       "  'test_auc': 0.7729926367461429,\n",
       "  'is_tuned': False},\n",
       " {'model_name': 'svm_base',\n",
       "  'model': 'svm',\n",
       "  'train_score': 0.9298984034833091,\n",
       "  'test_score': 0.8535478547854786,\n",
       "  'recall': 0.45161290322580644,\n",
       "  'specificity': 0.8752173913043478,\n",
       "  'precision': 0.16326530612244897,\n",
       "  'train_auc': 0.9783968162352202,\n",
       "  'test_auc': 0.777312412342216,\n",
       "  'is_tuned': False},\n",
       " {'model_name': 'xg_base',\n",
       "  'model': 'xg',\n",
       "  'train_score': 0.978011611030479,\n",
       "  'test_score': 0.9422442244224423,\n",
       "  'recall': 0.0967741935483871,\n",
       "  'specificity': 0.9878260869565217,\n",
       "  'precision': 0.3,\n",
       "  'train_auc': 0.9974362836276467,\n",
       "  'test_auc': 0.8528962131837308,\n",
       "  'is_tuned': False}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "412b542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input model\n",
    "def get_model_scores_roc(model_name,\n",
    "                     mod, \n",
    "                     mod_params={}, \n",
    "                     grid_search=False):\n",
    "    \n",
    "    \"\"\"Function accepts following inputs:\n",
    "    Name of model (str), model to be used (str), \n",
    "    model params(dict, optional), grid_seach(boolean, optional)\n",
    "    If grid_search is True, then please also input the relevant \n",
    "    params for GridSearching\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty dict for appending results\n",
    "    results = {}\n",
    "    \n",
    "    # instantiate pipe\n",
    "    pipe = Pipeline([\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    # check if GridSearch true or false\n",
    "    if grid_search:\n",
    "        \n",
    "        # combine vectorizer and mod params together\n",
    "        gs_params = {}\n",
    "        gs_params.update(mod_params)\n",
    "        \n",
    "        # instantiate GridSearchCV\n",
    "        gs = GridSearchCV(pipe, \n",
    "                          param_grid=gs_params,\n",
    "                          cv=5, \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1,\n",
    "                          scoring ='roc_auc'\n",
    "                          \n",
    "                         )\n",
    "        \n",
    "        # fit model\n",
    "        gs.fit(x_train_sc, y_train)\n",
    "        pipe = gs\n",
    "        \n",
    "    else:\n",
    "        # else fit model\n",
    "        pipe.fit(x_train_sc, y_train)\n",
    "    \n",
    "    # create predictions and confusion matrix\n",
    "    predictions = pipe.predict(x_test_sc)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    y_test_pred_prob = pipe.predict_proba(x_test_sc)[:,1]\n",
    "    y_train_pred_prob = pipe.predict_proba(x_train_sc)[:,1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    # Retrieve metrics and add to results\n",
    "    results['model_name'] = model_name\n",
    "    results['model'] = mod\n",
    "    results['train_score'] = pipe.score(x_train_sc, y_train)\n",
    "    results['test_score'] = pipe.score(x_test_sc, y_test)\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, \n",
    "                                     predictions)\n",
    "    \n",
    "    results['specificity'] = tn/(tn + fp)\n",
    "    \n",
    "    results['precision'] = precision_score(y_test, \n",
    "                                           predictions)\n",
    "    \n",
    "    results['train_auc'] = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    results['test_auc'] = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    results['is_tuned'] = grid_search\n",
    "    \n",
    "    if grid_search:\n",
    "        print('BEST PARAMS-->')\n",
    "        display(pipe.best_params_)\n",
    "    \n",
    "    # add results to list for model evaluation later\n",
    "    model_eval.append(results)\n",
    "    \n",
    "    print('METRICS-->')\n",
    "    display(results)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    \n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6973fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input model\n",
    "def get_model_scores_recall(model_name,\n",
    "                     mod, \n",
    "                     mod_params={}, \n",
    "                     grid_search=False):\n",
    "    \n",
    "    \"\"\"Function accepts following inputs:\n",
    "    Name of model (str), model to be used (str), \n",
    "    model params(dict, optional), grid_seach(boolean, optional)\n",
    "    If grid_search is True, then please also input the relevant \n",
    "    params for GridSearching\n",
    "    \"\"\"\n",
    "    \n",
    "    # empty dict for appending results\n",
    "    results = {}\n",
    "    \n",
    "    # instantiate pipe\n",
    "    pipe = Pipeline([\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    # check if GridSearch true or false\n",
    "    if grid_search:\n",
    "        \n",
    "        # combine vectorizer and mod params together\n",
    "        gs_params = {}\n",
    "        gs_params.update(mod_params)\n",
    "        \n",
    "        # instantiate GridSearchCV\n",
    "        gs = GridSearchCV(pipe, \n",
    "                          param_grid=gs_params,\n",
    "                          cv=5, \n",
    "                          verbose=2, \n",
    "                          n_jobs=-1,\n",
    "                          scoring ='recall'\n",
    "                          \n",
    "                         )\n",
    "        \n",
    "        # fit model\n",
    "        gs.fit(x_train_sc, y_train)\n",
    "        pipe = gs\n",
    "        \n",
    "    else:\n",
    "        # else fit model\n",
    "        pipe.fit(x_train_sc, y_train)\n",
    "    \n",
    "    # create predictions and confusion matrix\n",
    "    predictions = pipe.predict(x_test_sc)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    y_test_pred_prob = pipe.predict_proba(x_test_sc)[:,1]\n",
    "    y_train_pred_prob = pipe.predict_proba(x_train_sc)[:,1]\n",
    "\n",
    "    train_auc = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    test_auc = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    # Retrieve metrics and add to results\n",
    "    results['model_name'] = model_name\n",
    "    results['model'] = mod\n",
    "    results['train_score'] = pipe.score(x_train_sc, y_train)\n",
    "    results['test_score'] = pipe.score(x_test_sc, y_test)\n",
    "    \n",
    "    results['recall'] = recall_score(y_test, \n",
    "                                     predictions)\n",
    "    \n",
    "    results['specificity'] = tn/(tn + fp)\n",
    "    \n",
    "    results['precision'] = precision_score(y_test, \n",
    "                                           predictions)\n",
    "    \n",
    "    results['train_auc'] = roc_auc_score(y_train, y_train_pred_prob)\n",
    "    results['test_auc'] = roc_auc_score(y_test, y_test_pred_prob)\n",
    "    \n",
    "    results['is_tuned'] = grid_search\n",
    "    \n",
    "    if grid_search:\n",
    "        print('BEST PARAMS-->')\n",
    "        display(pipe.best_params_)\n",
    "    \n",
    "    # add results to list for model evaluation later\n",
    "    model_eval.append(results)\n",
    "    \n",
    "    print('METRICS-->')\n",
    "    display(results)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, \n",
    "                                      predictions).ravel()\n",
    "    \n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4d2330ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d6bd2abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    # Trying different types of regularization\n",
    "    'lr__penalty':['l2','l1', 'elasticnet'],\n",
    "\n",
    "     # Trying different alphas of: 1, 0.1, 0.05  (C = 1/alpha)\n",
    "    'lr__C':[1, 10, 20],\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "svc_params = {\n",
    "    'svm__C':[1, 10],\n",
    "    'svm__gamma':['scale', 'auto'], \n",
    "    'svm__kernel':['rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "xg_params = {\n",
    "    'xg__booster': ['gbtree', 'gblinear'],\n",
    "    'xg__scale_pos_weight': [0.5,1],\n",
    "    'xg__gamma': [0,1],\n",
    "    'xg__reg_alpha': [0, 1],\n",
    "    'xg__reg_lambda': [1, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "961f6d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 45.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.8808348  0.88250683        nan 0.88305926 0.88328855        nan\n",
      " 0.88320735 0.88332805        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS-->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 20, 'lr__penalty': 'l1'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'lr_tune',\n",
       " 'model': 'lr',\n",
       " 'train_score': 0.8894924071191288,\n",
       " 'test_score': 0.7765971248246846,\n",
       " 'recall': 0.5887096774193549,\n",
       " 'specificity': 0.7882608695652173,\n",
       " 'precision': 0.13035714285714287,\n",
       " 'train_auc': 0.8894924071191288,\n",
       " 'test_auc': 0.7765971248246846,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1813\n",
      "False Positives: 487\n",
      "False Negatives: 51\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "lr_tune = get_model_scores_roc('lr_tune', 'lr',lr_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "23374ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "15 fits failed out of a total of 45.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py\", line 394, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\", line 1471, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.8754717  0.87648766        nan 0.87692308 0.8754717         nan\n",
      " 0.87590711 0.87518142        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS-->\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 10, 'lr__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'lr_tune_recall',\n",
       " 'model': 'lr',\n",
       " 'train_score': 0.8795355587808418,\n",
       " 'test_score': 0.5887096774193549,\n",
       " 'recall': 0.5887096774193549,\n",
       " 'specificity': 0.7882608695652173,\n",
       " 'precision': 0.13035714285714287,\n",
       " 'train_auc': 0.8893119010955909,\n",
       " 'test_auc': 0.7761553295932678,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 1813\n",
      "False Positives: 487\n",
      "False Negatives: 51\n",
      "True Positives: 73\n"
     ]
    }
   ],
   "source": [
    "lr_tune_recall = get_model_scores_recall('lr_tune_recall', 'lr',lr_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8eb92931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST PARAMS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'svm_tune',\n",
       " 'model': 'svm',\n",
       " 'train_score': 0.9935818196372185,\n",
       " 'test_score': 0.7764638849929875,\n",
       " 'recall': 0.3548387096774194,\n",
       " 'specificity': 0.9265217391304348,\n",
       " 'precision': 0.20657276995305165,\n",
       " 'train_auc': 0.9935817353772005,\n",
       " 'test_auc': 0.7742917251051893,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2131\n",
      "False Positives: 169\n",
      "False Negatives: 80\n",
      "True Positives: 44\n"
     ]
    }
   ],
   "source": [
    "svm_tune = get_model_scores_roc('svm_tune', 'svm',svc_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a1b5aea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "BEST PARAMS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svm__C': 10, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'svm_tune_recall',\n",
       " 'model': 'svm',\n",
       " 'train_score': 0.997532656023222,\n",
       " 'test_score': 0.3548387096774194,\n",
       " 'recall': 0.3548387096774194,\n",
       " 'specificity': 0.9265217391304348,\n",
       " 'precision': 0.20657276995305165,\n",
       " 'train_auc': 0.9935817353772005,\n",
       " 'test_auc': 0.7742917251051893,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2131\n",
      "False Positives: 169\n",
      "False Negatives: 80\n",
      "True Positives: 44\n"
     ]
    }
   ],
   "source": [
    "svm_tune_recall = get_model_scores_recall('svm_tune_recall', 'svm',svc_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c02ab6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:40:47] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "BEST PARAMS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xg__booster': 'gbtree',\n",
       " 'xg__gamma': 1,\n",
       " 'xg__reg_alpha': 0,\n",
       " 'xg__reg_lambda': 1,\n",
       " 'xg__scale_pos_weight': 0.5}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'xg_tune',\n",
       " 'model': 'xg',\n",
       " 'train_score': 0.995142209845362,\n",
       " 'test_score': 0.8477857643758766,\n",
       " 'recall': 0.056451612903225805,\n",
       " 'specificity': 0.9947826086956522,\n",
       " 'precision': 0.3684210526315789,\n",
       " 'train_auc': 0.995142209845362,\n",
       " 'test_auc': 0.8477857643758766,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2288\n",
      "False Positives: 12\n",
      "False Negatives: 117\n",
      "True Positives: 7\n"
     ]
    }
   ],
   "source": [
    "xgb_tune = get_model_scores_roc('xg_tune', 'xg',xg_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "47eb2dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/user_1/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:37] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "BEST PARAMS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xg__booster': 'gbtree',\n",
       " 'xg__gamma': 0,\n",
       " 'xg__reg_alpha': 0,\n",
       " 'xg__reg_lambda': 1,\n",
       " 'xg__scale_pos_weight': 1}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS-->\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_name': 'xg_tune_recall',\n",
       " 'model': 'xg',\n",
       " 'train_score': 0.9631349782293178,\n",
       " 'test_score': 0.0967741935483871,\n",
       " 'recall': 0.0967741935483871,\n",
       " 'specificity': 0.9878260869565217,\n",
       " 'precision': 0.3,\n",
       " 'train_auc': 0.9974362836276467,\n",
       " 'test_auc': 0.8528962131837308,\n",
       " 'is_tuned': True}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 2272\n",
      "False Positives: 28\n",
      "False Negatives: 112\n",
      "True Positives: 12\n"
     ]
    }
   ],
   "source": [
    "xgb_tune_recall = get_model_scores_recall('xg_tune_recall', 'xg',xg_params,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "abacc5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>is_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_tune</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.889492</td>\n",
       "      <td>0.776597</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.788261</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.889492</td>\n",
       "      <td>0.776597</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_tune_recall</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.879536</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.788261</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.889312</td>\n",
       "      <td>0.776155</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm_tune</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.776464</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.926522</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.774292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svm_tune_recall</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.997533</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.926522</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.774292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xg_tune_recall</td>\n",
       "      <td>xg</td>\n",
       "      <td>0.963135</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.987826</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.852896</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xg_tune</td>\n",
       "      <td>xg</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.847786</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.994783</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.847786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_name model  train_score  test_score    recall  specificity  \\\n",
       "0          lr_tune    lr     0.889492    0.776597  0.588710     0.788261   \n",
       "1   lr_tune_recall    lr     0.879536    0.588710  0.588710     0.788261   \n",
       "2         svm_tune   svm     0.993582    0.776464  0.354839     0.926522   \n",
       "3  svm_tune_recall   svm     0.997533    0.354839  0.354839     0.926522   \n",
       "5   xg_tune_recall    xg     0.963135    0.096774  0.096774     0.987826   \n",
       "4          xg_tune    xg     0.995142    0.847786  0.056452     0.994783   \n",
       "\n",
       "   precision  train_auc  test_auc  is_tuned  \n",
       "0   0.130357   0.889492  0.776597      True  \n",
       "1   0.130357   0.889312  0.776155      True  \n",
       "2   0.206573   0.993582  0.774292      True  \n",
       "3   0.206573   0.993582  0.774292      True  \n",
       "5   0.300000   0.997436  0.852896      True  \n",
       "4   0.368421   0.995142  0.847786      True  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_eval).sort_values('recall',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "68209043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>model</th>\n",
       "      <th>train_score</th>\n",
       "      <th>test_score</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>precision</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>is_tuned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xg_tune_recall</td>\n",
       "      <td>xg</td>\n",
       "      <td>0.963135</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.987826</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.997436</td>\n",
       "      <td>0.852896</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xg_tune</td>\n",
       "      <td>xg</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.847786</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.994783</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.995142</td>\n",
       "      <td>0.847786</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_tune</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.889492</td>\n",
       "      <td>0.776597</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.788261</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.889492</td>\n",
       "      <td>0.776597</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lr_tune_recall</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.879536</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.588710</td>\n",
       "      <td>0.788261</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.889312</td>\n",
       "      <td>0.776155</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm_tune</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.776464</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.926522</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.774292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>svm_tune_recall</td>\n",
       "      <td>svm</td>\n",
       "      <td>0.997533</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.926522</td>\n",
       "      <td>0.206573</td>\n",
       "      <td>0.993582</td>\n",
       "      <td>0.774292</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        model_name model  train_score  test_score    recall  specificity  \\\n",
       "5   xg_tune_recall    xg     0.963135    0.096774  0.096774     0.987826   \n",
       "4          xg_tune    xg     0.995142    0.847786  0.056452     0.994783   \n",
       "0          lr_tune    lr     0.889492    0.776597  0.588710     0.788261   \n",
       "1   lr_tune_recall    lr     0.879536    0.588710  0.588710     0.788261   \n",
       "2         svm_tune   svm     0.993582    0.776464  0.354839     0.926522   \n",
       "3  svm_tune_recall   svm     0.997533    0.354839  0.354839     0.926522   \n",
       "\n",
       "   precision  train_auc  test_auc  is_tuned  \n",
       "5   0.300000   0.997436  0.852896      True  \n",
       "4   0.368421   0.995142  0.847786      True  \n",
       "0   0.130357   0.889492  0.776597      True  \n",
       "1   0.130357   0.889312  0.776155      True  \n",
       "2   0.206573   0.993582  0.774292      True  \n",
       "3   0.206573   0.993582  0.774292      True  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(model_eval).sort_values('test_auc',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b078ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
